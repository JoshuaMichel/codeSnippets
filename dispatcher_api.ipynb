{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the API calls the flow on a input row of data to get\n",
    "# a prediction value from the ML models\n",
    "# The api_py_funtion is the code that will run on the API\n",
    "# deployed onto kubernetes.  The rest of the code is stored\n",
    "# elsewhere\n",
    "import api.dispatcher_functions as dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection\n",
    "def load_project(projectKey,apiKey):\n",
    "    dataiku.set_remote_dss(\"url\", apiKey)\n",
    "    client = dataiku.api_client()\n",
    "    \n",
    "    project = client.get_project(projectKey)\n",
    "    \n",
    "    project_string = project.project_key + '.'\n",
    "    \n",
    "    jobId=0\n",
    "    \n",
    "    return inputDb, project, project_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the handles on the data\n",
    "def getData(inputDb, project, project_string, data):\n",
    "#     Running Random record dataset\n",
    "    recipeObj = project.get_recipe('RECIPE NODE NAME HERE')\n",
    "    recipeObj.run()\n",
    "    recipeObj = project.get_recipe('RECIPE NODE NAME HERE')\n",
    "    recipeObj.run()\n",
    "\n",
    "#     ### START PYTHON COMPUTE RECIPE ###\n",
    "    debug = 0\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    ds = dataiku.Dataset(project_string + inputDb)\n",
    "    df = ds.get_dataframe()\n",
    "    \n",
    "    ds_m = dataiku.Dataset(\"DATASET NAME HERE\")\n",
    "    mfdf = ds_m.get_dataframe()\n",
    "\n",
    "    folder = dataiku.Folder(\"FOLDER NODE NAME HERE\")\n",
    "    folder_info = folder.get_info()\n",
    "\n",
    "    with folder.get_download_stream(\"folder.json\") as f:\n",
    "        jsonData = json.load(f)\n",
    "    load_time = str(time.perf_counter() - start)\n",
    "    \n",
    "    return df, jsonData, mfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clean transformations on data\n",
    "def data_engineer(df, jsonData, mfdf):\n",
    "    # doing data engineering\n",
    "    temp = time.perf_counter()\n",
    "\n",
    "    final_object = engineer.vectorize(df,jsonData,mfdf)\n",
    "    select_final = final_object.final_data.copy()\n",
    "    de_time = str(time.perf_counter() - temp)\n",
    "\n",
    "    return select_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data through the flow\n",
    "def inference_pipeline(select_final):\n",
    "    temp = time.perf_counter()\n",
    "    cols = list(select_final.columns)\n",
    "    for i in cols:\n",
    "        select_final[i] = select_final[i].apply(lambda x: ''.join(str(x)))\n",
    "\n",
    "    jobId = select_final['job_id'][0]\n",
    "    prediction_object = inference.Inference_pipeline_batch(select_final, model_dict.model_dict)\n",
    "\n",
    "    return prediction_object, jobId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the final prediction response\n",
    "def final_format(prediction_object):\n",
    "    prediction_final = prediction_object.final_data.copy()\n",
    "    prediction_final.columns = prediction_final.columns.str.replace(',', '_')\n",
    "\n",
    "    prediction_final['final_prediction'] = prediction_final['cc_prediction']+ \"_\" + prediction_final['ac_prediction'] + \"_\" + prediction_final['fpc'] + ' ' + prediction_final['cc_prediction'] + \"_\" + prediction_final['ac_prediction'] + \"_\" + prediction_final['spc'] + ' ' + prediction_final['cc_prediction'] + \"_\" + prediction_final['ac_prediction'] + \"_\" + prediction_final['tpc']\n",
    "    return prediction_final._get_value(0,'final_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code on API dispatcher in kubernetes\n",
    "# Calls the above functions in consecutive order\n",
    "def api_py_function(projectKey, apiKey, data, inputDb):\n",
    "    project, project_string = load_project(projectKey,apiKey)\n",
    "    df, jsonData, mfdf = getData(inputDb, project, project_string, data)\n",
    "    select_final = data_engineer(df, jsonData, mfdf)\n",
    "    prediction_object, jobId = inference_pipeline(select_final)\n",
    "    prediction = final_format(prediction_object)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'PLACE DATA PREDICTION ROW HERE'\n",
    "inputDb = 'DATASET NAME HERE'\n",
    "projectKey = 'PROJECT KEY HERE'\n",
    "# Call the dispatcher function\n",
    "print(api_py_function(projectKey, apiKey, data, inputDb))"
   ]
  }
 ],
 "metadata": {
  "createdOn": 1692711764947,
  "creator": "600235209",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python in stg_maintenance_medium (env Maintenance_guided_repair_training_37)",
   "language": "python",
   "name": "py-dku-containerized-venv-maintenance_guided_repair_training_37-stg_maintenance_medium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "modifiedBy": "600235209",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
